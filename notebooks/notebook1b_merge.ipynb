{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fa83b7",
   "metadata": {},
   "source": [
    "# Part 1b: Data Integration and Predictor Engineering\n",
    "\n",
    "## From Raw Corpus to Powerful Predictors\n",
    "\n",
    "**Learning Objectives:**\n",
    "- **Process Raw Text**: Take a large, unstructured text corpus and turn it into a clean, tokenized list of words.\n",
    "- **Calculate Frequency**: Compute raw word frequency counts from the tokenized text.\n",
    "- **Integrate External Data**: Merge the LLM-derived frequencies with established psycholinguistic datasets (ECP, SUBTLEX) to create a rich, comparative dataset.\n",
    "- **Apply Transformations**: Convert raw frequencies into meaningful, psycholinguistically-validated scales (Schepens, Zipf).\n",
    "- **Export for Analysis**: Save the final, merged data into a single file, ready for statistical analysis in Notebook 2.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Research Context:** A raw text file isn't useful for statistical modeling. We need to \"engineer\" predictors from it. This notebook automates the critical pipeline from text to data. We will calculate our own `llm_frequency` and then place it alongside well-known, human-validated measures. This allows us to directly compare our LLM-based predictor with the \"gold standards\" in the field.\n",
    "\n",
    "# Prepare Predictors from Corpus\n",
    "\n",
    "This notebook processes the large corpus text file to extract word frequencies and prepare predictor data for analysis. The output will be used by `notebook2_corpus_analysis.ipynb` to compare different frequency measures.\n",
    "\n",
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and define the file paths for our input (the raw text corpus) and our output (the final CSV file with all predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc6dc38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting corpus processing...\n",
      "Corpus file: ../output/large_corpus.txt\n",
      "Output file: ../output/generated_corpus_with_predictors.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "corpus_file_path = '../output/large_corpus.txt'\n",
    "output_csv_path = '../output/generated_corpus_with_predictors.csv'\n",
    "\n",
    "print(\"Starting corpus processing...\")\n",
    "print(f\"Corpus file: {corpus_file_path}\")\n",
    "print(f\"Output file: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26ee0",
   "metadata": {},
   "source": [
    "## 2. Load and Validate the Corpus\n",
    "\n",
    "Before we can process the text, we need to load it into memory. We'll also perform a quick check to ensure the file exists and display the first few characters to confirm it has loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8796dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Corpus file found\n",
      "Reading corpus...\n",
      "Corpus loaded: 16,404,292 characters\n",
      "First 200 characters: **How to Create a Reliable Backup System for Your Digital Life**\n",
      "\n",
      "In today‚Äôs digital world, our lives are increasingly stored on computers, smartphones, and cloud services. From family photos and impo...\n",
      "Corpus loaded: 16,404,292 characters\n",
      "First 200 characters: **How to Create a Reliable Backup System for Your Digital Life**\n",
      "\n",
      "In today‚Äôs digital world, our lives are increasingly stored on computers, smartphones, and cloud services. From family photos and impo...\n"
     ]
    }
   ],
   "source": [
    "# Check if corpus file exists\n",
    "if not os.path.exists(corpus_file_path):\n",
    "    print(f\"ERROR: Corpus file not found at {corpus_file_path}\")\n",
    "    print(\"Please ensure the large_corpus.txt file exists in the output directory.\")\n",
    "else:\n",
    "    print(\"‚úì Corpus file found\")\n",
    "    \n",
    "    # Read the corpus\n",
    "    print(\"Reading corpus...\")\n",
    "    with open(corpus_file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus_text = f.read()\n",
    "    \n",
    "    print(f\"Corpus loaded: {len(corpus_text):,} characters\")\n",
    "    print(f\"First 200 characters: {corpus_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a9204",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing and Tokenization\n",
    "\n",
    "This is a critical step in turning unstructured text into data. We will:\n",
    "1.  **Clean the Text**: Remove any metadata comments that were added during the generation process.\n",
    "2.  **Lowercase**: Convert all text to lowercase to ensure that words like \"The\" and \"the\" are treated as the same token.\n",
    "3.  **Tokenize**: Use a regular expression (`regex`) to find all sequences of letters, effectively splitting the text into a list of words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8f0d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and tokenizing text...\n",
      "Total tokens extracted: 2,050,703\n",
      "Sample tokens: ['how', 'to', 'create', 'a', 'reliable', 'backup', 'system', 'for', 'your', 'digital', 'life', 'in', 'today', 's', 'digital', 'world', 'our', 'lives', 'are', 'increasingly']\n",
      "Total tokens extracted: 2,050,703\n",
      "Sample tokens: ['how', 'to', 'create', 'a', 'reliable', 'backup', 'system', 'for', 'your', 'digital', 'life', 'in', 'today', 's', 'digital', 'world', 'our', 'lives', 'are', 'increasingly']\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing and tokenization\n",
    "print(\"Cleaning and tokenizing text...\")\n",
    "\n",
    "# Remove metadata comments before processing\n",
    "# The DOTALL flag is crucial for multiline JSON\n",
    "cleaned_text = re.sub(r'<!-- Story Metadata:.*?-->', '', corpus_text, flags=re.DOTALL)\n",
    "\n",
    "# Convert to lowercase and extract words using regex\n",
    "# This pattern matches sequences of letters and some common contractions\n",
    "words = re.findall(r\"\\b[a-z]+(?:'[a-z]+)?\\b\", cleaned_text.lower())\n",
    "\n",
    "print(f\"Total tokens extracted: {len(words):,}\")\n",
    "print(f\"Sample tokens: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69af00",
   "metadata": {},
   "source": [
    "## 4. Calculate Word Frequencies\n",
    "\n",
    "Now that we have a clean list of tokens, we can calculate the frequency of each word. We'll use Python's `Counter` to create a dictionary where keys are words and values are their raw counts.\n",
    "\n",
    "This raw count is the simplest form of a frequency predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word frequencies...\n",
      "Unique words: 46,483\n",
      "Most common words: [('the', 113290), ('a', 85845), ('and', 44435), ('of', 43403), ('to', 34483), ('in', 33797), ('it', 32853), ('s', 28349), ('like', 17852), ('that', 16510)]\n",
      "Unique words: 46,483\n",
      "Most common words: [('the', 113290), ('a', 85845), ('and', 44435), ('of', 43403), ('to', 34483), ('in', 33797), ('it', 32853), ('s', 28349), ('like', 17852), ('that', 16510)]\n"
     ]
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "print(\"Counting word frequencies...\")\n",
    "word_counts = Counter(words)\n",
    "total_words = len(words)\n",
    "unique_words = len(word_counts)\n",
    "\n",
    "print(f\"Unique words: {unique_words:,}\")\n",
    "print(f\"Most common words: {word_counts.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfbaff",
   "metadata": {},
   "source": [
    "## 5. Create a Structured DataFrame\n",
    "\n",
    "To make the data easier to work with, we'll convert our word counts into a `pandas` DataFrame. This structure allows for powerful data manipulation, filtering, and merging. We'll also add a `word_length` column, which is another simple but powerful predictor of reading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame...\n",
      "DataFrame created with 46483 words.\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with word frequency data\n",
    "print(\"Creating DataFrame...\")\n",
    "\n",
    "# Convert word counts to DataFrame\n",
    "df_words = pd.DataFrame(word_counts.items(), columns=['word', 'llm_frequency_raw'])\n",
    "df_words['word_length'] = df_words['word'].apply(len)\n",
    "\n",
    "print(f\"DataFrame created with {len(df_words)} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5379e",
   "metadata": {},
   "source": [
    "## 6. Data Integration: Merging with External Datasets\n",
    "\n",
    "**This is where we create a rich dataset for comparison.** Our LLM-generated frequency is interesting, but its true value is only revealed when compared against established, human-derived measures.\n",
    "\n",
    "We will merge our `llm_frequency_raw` data with two key external datasets:\n",
    "1.  **The English Crowdsourcing Project (ECP)**: This dataset provides human reading time data and contains several pre-computed frequency measures (`SUBTLEX`, `Multilex`) and even a familiarity rating derived from GPT (`GPT`).\n",
    "2.  **SUBTLEX-US**: While ECP gives us the final *Zipf-scaled* SUBTLEX value, it doesn't give us the *raw frequency count*. We load the original SUBTLEX-US corpus data to get this raw count, which is essential for applying our own transformations consistently.\n",
    "\n",
    "By merging these, we can place our new predictor (`llm_frequency`) in the same rows as the established ones, setting the stage for a direct comparison in Notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrating with ECP reference data...\n",
      "‚úÖ Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "‚úÖ Loaded 74286 records from SUBTLEX-US dataset\n",
      "‚úÖ Merged generated data with ECP reference measures and SUBTLEX-US raw frequencies.\n",
      "‚úÖ Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "‚úÖ Loaded 74286 records from SUBTLEX-US dataset\n",
      "‚úÖ Merged generated data with ECP reference measures and SUBTLEX-US raw frequencies.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Integration ---\n",
    "print(\"\\nIntegrating with ECP reference data...\")\n",
    "try:\n",
    "    ecp_df = pd.read_csv('../data/lexicaldecision/ecp/English Crowdsourcing Project All Native Speakers.csv')\n",
    "    print(f\"‚úÖ Loaded {len(ecp_df)} records from ECP dataset\")\n",
    "    \n",
    "    # Load SUBTLEX-US data to get raw frequency counts\n",
    "    print(\"Loading SUBTLEX-US data for raw frequency counts...\")\n",
    "    subtlex_us_path = '../data/frequency/subtlex-us/SUBTLEXus74286wordstextversion.txt'\n",
    "    subtlex_df = pd.read_csv(subtlex_us_path, sep='\t')\n",
    "    print(f\"‚úÖ Loaded {len(subtlex_df)} records from SUBTLEX-US dataset\")\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    subtlex_df = subtlex_df.rename(columns={\n",
    "        'Word': 'word',\n",
    "        'FREQcount': 'subtlex_freq_raw'\n",
    "    })\n",
    "    \n",
    "    # Merge ECP data with SUBTLEX-US data to get raw frequency counts\n",
    "    ecp_with_subtlex = pd.merge(ecp_df, subtlex_df[['word', 'subtlex_freq_raw']], \n",
    "                               left_on='spelling' if 'spelling' in ecp_df.columns else 'Word', \n",
    "                               right_on='word', how='left')\n",
    "    \n",
    "    # Define word column and predictors to merge\n",
    "    word_col = 'spelling' if 'spelling' in ecp_with_subtlex.columns else 'Word'\n",
    "    # We need the raw SUBTLEX frequency count (subtlex_freq_raw) and the total corpus size for the Schepens transform.\n",
    "    # The 'SUBTLEX' column is the Zipf scale, which we'll also use.\n",
    "    ref_cols = ['SUBTLEX', 'subtlex_freq_raw', 'Multilex', 'GPT']\n",
    "    cols_to_merge = [word_col] + [col for col in ref_cols if col in ecp_with_subtlex.columns]\n",
    "    \n",
    "    # Merge generated frequencies with reference data\n",
    "    merged_df = pd.merge(df_words, ecp_with_subtlex[cols_to_merge], left_on='word', right_on=word_col, how='left')\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    merged_df = merged_df.rename(columns={\n",
    "        'SUBTLEX': 'subtlex_zipf', # This is the pre-computed Zipf scale\n",
    "        'subtlex_freq_raw': 'subtlex_freq_raw', # This is the raw frequency count from SUBTLEX-US\n",
    "        'Multilex': 'multilex_zipf',\n",
    "        'GPT': 'gpt_familiarity'\n",
    "    })\n",
    "    if word_col != 'word':\n",
    "        merged_df = merged_df.drop(columns=[word_col])\n",
    "        \n",
    "    print(\"‚úÖ Merged generated data with ECP reference measures and SUBTLEX-US raw frequencies.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Data file not found: {e}. Proceeding without reference measures.\")\n",
    "    merged_df = df_words.copy()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading reference data: {e}. Proceeding without reference measures.\")\n",
    "    merged_df = df_words.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be143e7c",
   "metadata": {},
   "source": [
    "## 7. Logarithmic Transformations: Creating Comparable Predictors\n",
    "\n",
    "**Why transform raw frequencies?** Raw frequency counts are heavily skewed (a few words like \"the\" appear millions of times, while most words appear very rarely). This skew violates the assumptions of many statistical models (like linear regression). Logarithmic transformations compress the scale, making the distribution more normal and better behaved for statistical analysis.\n",
    "\n",
    "We will apply two important transformations to both our LLM frequencies and the SUBTLEX frequencies:\n",
    "\n",
    "1.  **Schepens et al. Transformation**: `log( (1 + frequency_raw) * 1e6 / corpus_size )`\n",
    "    - This is the formula used in the foundational paper for this project. It scales the frequency relative to the corpus size.\n",
    "\n",
    "2.  **Van Heuven et al. (Zipf) Transformation**: `log10((raw_frequency + 1) / (corpus_M + types_M)) + 3`\n",
    "    - This is a widely-used standard in psycholinguistics (used by the ECP). It accounts for both the total number of words (corpus size) and the number of unique words (types).\n",
    "\n",
    "By applying these formulas to both our LLM corpus and the SUBTLEX corpus, we create four key predictors that can be directly compared:\n",
    "- `llm_freq_schepens`\n",
    "- `llm_freq_zipf`\n",
    "- `subtlex_schepens` (calculated by us from raw SUBTLEX counts)\n",
    "- `subtlex_zipf` (the original value from ECP, which we confirmed uses the Van Heuven formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "g8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying logarithmic transformations for direct comparison...\n",
      "LLM Corpus Size: 2,050,703 words\n",
      "SUBTLEX-US Corpus Size: 51,000,000 words\n",
      "   ‚úì Calculated Schepens and Zipf scales for LLM frequency\n",
      "   ‚úì Calculated Schepens scale for SUBTLEX frequency\n",
      "   ‚úì Calculated Van Heuven Zipf scale for SUBTLEX frequency\n",
      "\n",
      "‚úÖ Transformations complete.\n",
      "\n",
      "First 10 rows of the processed data:\n",
      "   word  llm_freq_schepens  llm_freq_zipf  subtlex_zipf  subtlex_schepens  \\\n",
      "0   the          10.919532       7.732558      7.468478         10.290422   \n",
      "1     a          10.642128       7.612083      7.309360          9.924040   \n",
      "2   and           9.983623       7.326098      7.126116          9.502104   \n",
      "3    of           9.960124       7.315893      7.063010          9.356798   \n",
      "4    to           9.730068       7.215981      7.355006         10.029145   \n",
      "5    in           9.709974       7.207254      6.989451          9.187423   \n",
      "6    it           9.681646       7.194951      7.275782          9.846723   \n",
      "7     s           9.534200       7.130916      7.316033          9.939405   \n",
      "8  like           9.071744       6.930074      6.601354          8.293795   \n",
      "9  that           8.993599       6.896136      7.148972          9.554734   \n",
      "\n",
      "   subtlex_zipf_vanheuven  \n",
      "0                7.468441  \n",
      "1                7.309324  \n",
      "2                7.126079  \n",
      "3                7.062973  \n",
      "4                7.354970  \n",
      "5                6.989415  \n",
      "6                7.275745  \n",
      "7                7.315997  \n",
      "8                6.601317  \n",
      "9                7.148936  \n",
      "\n",
      "‚úÖ Transformations complete.\n",
      "\n",
      "First 10 rows of the processed data:\n",
      "   word  llm_freq_schepens  llm_freq_zipf  subtlex_zipf  subtlex_schepens  \\\n",
      "0   the          10.919532       7.732558      7.468478         10.290422   \n",
      "1     a          10.642128       7.612083      7.309360          9.924040   \n",
      "2   and           9.983623       7.326098      7.126116          9.502104   \n",
      "3    of           9.960124       7.315893      7.063010          9.356798   \n",
      "4    to           9.730068       7.215981      7.355006         10.029145   \n",
      "5    in           9.709974       7.207254      6.989451          9.187423   \n",
      "6    it           9.681646       7.194951      7.275782          9.846723   \n",
      "7     s           9.534200       7.130916      7.316033          9.939405   \n",
      "8  like           9.071744       6.930074      6.601354          8.293795   \n",
      "9  that           8.993599       6.896136      7.148972          9.554734   \n",
      "\n",
      "   subtlex_zipf_vanheuven  \n",
      "0                7.468441  \n",
      "1                7.309324  \n",
      "2                7.126079  \n",
      "3                7.062973  \n",
      "4                7.354970  \n",
      "5                6.989415  \n",
      "6                7.275745  \n",
      "7                7.315997  \n",
      "8                6.601317  \n",
      "9                7.148936  \n"
     ]
    }
   ],
   "source": [
    "# --- Logarithmic Transformations ---\n",
    "print(\"\\nApplying logarithmic transformations for direct comparison...\")\n",
    "\n",
    "# Define corpus sizes for Schepens calculation\n",
    "# SUBTLEX-US corpus size is approximately 51 million words (using the same as UK for consistency)\n",
    "SUBTLEX_US_SIZE = 51_000_000\n",
    "llm_corpus_size = total_words\n",
    "print(f\"LLM Corpus Size: {llm_corpus_size:,} words\")\n",
    "print(f\"SUBTLEX-US Corpus Size: {SUBTLEX_US_SIZE:,} words\")\n",
    "\n",
    "# Define transformation functions\n",
    "def schepens_log(freq_series_raw, corpus_size):\n",
    "    \"\"\"log( (1 + frequency_raw) * 1e6 / corpus_size )\"\"\"\n",
    "    # Using log1p is more numerically stable for log(1 + x)\n",
    "    return np.log1p(freq_series_raw) + np.log(1_000_000 / corpus_size)\n",
    "\n",
    "def van_heuven_zipf_wrong(freq_series_raw, corpus_size):\n",
    "    \"\"\"log10(frequency_per_million + 1)\"\"\"\n",
    "    freq_per_million = (freq_series_raw / corpus_size) * 1_000_000\n",
    "    return np.log10(freq_per_million + 1)\n",
    "\n",
    "def van_heuven_zipf(freq_series_raw, corpus_size, word_types):\n",
    "    \"\"\"log10((raw_frequency + 1) / (corpus_size_in_millions + word_types_in_millions)) + 3\"\"\"\n",
    "    corpus_size_millions = corpus_size / 1_000_000\n",
    "    word_types_millions = word_types / 1_000_000\n",
    "    \n",
    "    # Add 1 to frequency to handle 0 values\n",
    "    numerator = freq_series_raw + 1\n",
    "    denominator = corpus_size_millions + word_types_millions\n",
    "    \n",
    "    # Avoid division by zero if denominator is 0\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    return np.log10(numerator / denominator) + 3\n",
    "\n",
    "# --- Apply transformations to create the four target measures ---\n",
    "\n",
    "# 1. LLM-derived frequencies\n",
    "merged_df['llm_freq_schepens'] = schepens_log(merged_df['llm_frequency_raw'], llm_corpus_size)\n",
    "merged_df['llm_freq_zipf'] = van_heuven_zipf(merged_df['llm_frequency_raw'], llm_corpus_size, unique_words)\n",
    "print(\"   ‚úì Calculated Schepens and Zipf scales for LLM frequency\")\n",
    "\n",
    "# 2. SUBTLEX frequencies\n",
    "if 'subtlex_freq_raw' in merged_df.columns:\n",
    "    # We calculate the Schepens scale from the raw SUBTLEX frequency.\n",
    "    merged_df['subtlex_schepens'] = schepens_log(merged_df['subtlex_freq_raw'], SUBTLEX_US_SIZE)\n",
    "    \n",
    "    # Also calculate Van Heuven Zipf scale for SUBTLEX using proper SUBTLEX parameters\n",
    "    SUBTLEX_WORD_TYPES = 74286  # Number of word types in SUBTLEX-US dataset\n",
    "    merged_df['subtlex_zipf_vanheuven'] = van_heuven_zipf(merged_df['subtlex_freq_raw'], SUBTLEX_US_SIZE, SUBTLEX_WORD_TYPES)\n",
    "    \n",
    "    print(\"   ‚úì Calculated Schepens scale for SUBTLEX frequency\")\n",
    "    print(\"   ‚úì Calculated Van Heuven Zipf scale for SUBTLEX frequency\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è 'subtlex_freq_raw' column not found. Cannot calculate subtlex_schepens.\")\n",
    "    merged_df['subtlex_schepens'] = np.nan\n",
    "    merged_df['subtlex_zipf_vanheuven'] = np.nan\n",
    "\n",
    "# Sort by raw frequency\n",
    "merged_df = merged_df.sort_values('llm_frequency_raw', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n‚úÖ Transformations complete.\")\n",
    "print(\"\\nFirst 10 rows of the processed data:\")\n",
    "display_cols = ['word', 'llm_freq_schepens', 'llm_freq_zipf', 'subtlex_zipf', 'subtlex_schepens', 'subtlex_zipf_vanheuven']\n",
    "available_cols = [col for col in display_cols if col in merged_df.columns]\n",
    "print(merged_df[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0896c6f",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "h8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed data saved to ../output/merged_predictors.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save Processed Data ---\n",
    "output_path = '../output/merged_predictors.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Processed data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6902f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
