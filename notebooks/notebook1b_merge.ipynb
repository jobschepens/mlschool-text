{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup (Run this first on Colab/Binder)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸ”§ Setting up Google Colab environment...\")\n",
    "    # Clone the repository if not already present\n",
    "    if not os.path.exists('mlschool-text'):\n",
    "        !git clone https://github.com/jobschepens/mlschool-text.git\n",
    "        os.chdir('mlschool-text')\n",
    "    else:\n",
    "        os.chdir('mlschool-text')\n",
    "    \n",
    "    # Install requirements\n",
    "    !pip install -q -r requirements_colab.txt\n",
    "    print(\"âœ… Colab setup complete!\")\n",
    "\n",
    "elif 'BINDER_LAUNCH_HOST' in os.environ:\n",
    "    print(\"ðŸ”§ Binder environment detected - dependencies should already be installed\")\n",
    "    print(\"âœ… Binder setup complete!\")\n",
    "\n",
    "elif 'CODESPACES' in os.environ:\n",
    "    print(\"ðŸš€ GitHub Codespaces environment detected\")\n",
    "    print(\"Dependencies should be installed automatically via devcontainer.json\")\n",
    "    print(\"âœ… Codespaces setup complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ’» Local environment detected\")\n",
    "    print(\"Make sure you've run: pip install -r requirements.txt\")\n",
    "\n",
    "# Set working directory for consistent paths\n",
    "if os.path.exists('mlschool-text') and not os.getcwd().endswith('mlschool-text'):\n",
    "    os.chdir('mlschool-text')\n",
    "elif os.getcwd().endswith('notebooks'):\n",
    "    # If we're in the notebooks directory, go up one level\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"ðŸ“ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify key files are accessible\n",
    "key_files = ['models.json', 'data/', 'scripts/', 'output/']\n",
    "missing_files = []\n",
    "for file_path in key_files:\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"âš ï¸ Warning: Cannot find {missing_files}\")\n",
    "    print(\"ðŸ’¡ Make sure you're in the correct directory\")\n",
    "else:\n",
    "    print(\"âœ… All key project files accessible\")\n",
    "\n",
    "print(\"ðŸŽ¯ Ready to start! You can now run the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa83b7",
   "metadata": {},
   "source": [
    "# Part 1b: Data Integration and Predictor Engineering\n",
    "\n",
    "## From Raw Corpus to Powerful Predictors\n",
    "\n",
    "**Learning Objectives:**\n",
    "- **Process Raw Text**: Take a large, unstructured text corpus and turn it into a clean, tokenized list of words.\n",
    "- **Calculate Frequency**: Compute raw word frequency counts from the tokenized text.\n",
    "- **Integrate External Data**: Merge the LLM-derived frequencies with established psycholinguistic datasets (ECP, SUBTLEX) to create a rich, comparative dataset.\n",
    "- **Apply Transformations**: Convert raw frequencies into meaningful, psycholinguistically-validated scales (Schepens, Zipf).\n",
    "- **Export for Analysis**: Save the final, merged data into a single file, ready for statistical analysis in Notebook 2.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **Research Context:** A raw text file isn't useful for statistical modeling. We need to \"engineer\" predictors from it. This notebook automates the critical pipeline from text to data. We will calculate our own `llm_frequency` and then place it alongside well-known, human-validated measures. This allows us to directly compare our LLM-based predictor with the \"gold standards\" in the field.\n",
    "\n",
    "# Prepare Predictors from Corpus\n",
    "\n",
    "This notebook processes the large corpus text file to extract word frequencies and prepare predictor data for analysis. The output will be used by `notebook2_corpus_analysis.ipynb` to compare different frequency measures.\n",
    "\n",
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and define the file paths for our input (the raw text corpus) and our output (the final CSV file with all predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc6dc38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Path utilities not found, using fallback functions\n",
      "Starting corpus processing...\n",
      "Corpus file: ..\\output\\large_corpus.txt\n",
      "Output file: ..\\output\\generated_corpus_with_predictors.csv\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to Python path for imports\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import path utilities for cross-platform compatibility\n",
    "try:\n",
    "    from path_utils import get_project_path, get_output_path\n",
    "    print(\"âœ… Path utilities loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Path utilities not found, using fallback functions\")\n",
    "    \n",
    "    def get_project_path(relative_path):\n",
    "        \"\"\"Fallback path utility function\"\"\"\n",
    "        if os.path.exists(relative_path):\n",
    "            return relative_path\n",
    "        parent_path = os.path.join('..', relative_path)\n",
    "        if os.path.exists(parent_path):\n",
    "            return parent_path\n",
    "        return relative_path\n",
    "    \n",
    "    def get_output_path(filename):\n",
    "        \"\"\"Fallback output path utility function\"\"\"\n",
    "        output_dir = get_project_path('output')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        return os.path.join(output_dir, filename)\n",
    "\n",
    "# File paths using path utilities\n",
    "corpus_file_path = get_output_path('large_corpus.txt')\n",
    "output_csv_path = get_output_path('generated_corpus_with_predictors.csv')\n",
    "\n",
    "print(\"Starting corpus processing...\")\n",
    "print(f\"Corpus file: {corpus_file_path}\")\n",
    "print(f\"Output file: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26ee0",
   "metadata": {},
   "source": [
    "## 2. Corpus Selection\n",
    "\n",
    "Choose which corpus file you want to process for your analysis. The notebook will:\n",
    "\n",
    "1. **Detect Available Corpus Files**: Automatically find all `.txt` corpus files in the output directory\n",
    "2. **Let You Choose**: Select which specific corpus file to process\n",
    "3. **Generate Standard Variables**: Create the standard `llm_frequency_raw` and transformation variables\n",
    "\n",
    "This simplified approach creates clean, focused analysis with a single chosen corpus for easier interpretation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8796dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Available corpus files in output directory:\n",
      "   1. large_corpus_2m_llama_20250908_002336.txt (3.9 MB)\n",
      "   2. large_corpus_2m_qwen_dynamic_20250908_002429.txt (15.1 MB)\n",
      "   3. large_corpus_2m_qwen_seeds.txt (16.0 MB)\n",
      "   4. large_corpus_gpt-oss-20b_dynamic_20250908_002239.txt (0.8 MB)\n",
      "\n",
      "ðŸ“Š Found 4 corpus file(s)\n",
      "\n",
      "ðŸŽ¯ Selected corpus file: large_corpus_2m_qwen_dynamic_20250908_002429.txt\n",
      "\n",
      "ðŸŽ¯ Selected corpus file: large_corpus_2m_qwen_dynamic_20250908_002429.txt\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for corpus processing\n",
    "import re\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "# Discover available corpus files\n",
    "output_dir = get_output_path('')  # Get the output directory path\n",
    "corpus_pattern = os.path.join(output_dir, \"*.txt\")\n",
    "available_corpus_files = glob.glob(corpus_pattern)\n",
    "\n",
    "# Filter to only include actual corpus files (exclude metadata files, etc.)\n",
    "corpus_files = []\n",
    "for file_path in available_corpus_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    # Include files that look like corpus files\n",
    "    if (filename.startswith('large_corpus') or \n",
    "        'corpus' in filename.lower() and not filename.endswith('_metadata.txt')):\n",
    "        corpus_files.append(file_path)\n",
    "\n",
    "print(\"ðŸ” Available corpus files in output directory:\")\n",
    "for i, file_path in enumerate(corpus_files, 1):\n",
    "    filename = os.path.basename(file_path)\n",
    "    try:\n",
    "        # Get file size for reference\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   {i}. {filename} ({size_mb:.1f} MB)\")\n",
    "    except:\n",
    "        print(f\"   {i}. {filename}\")\n",
    "\n",
    "if not corpus_files:\n",
    "    print(\"âŒ No corpus files found in the output directory.\")\n",
    "    print(\"Please ensure you have generated corpus files first using the generation scripts.\")\n",
    "    selected_file = None\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Found {len(corpus_files)} corpus file(s)\")\n",
    "    \n",
    "    # Simple selection: choose which file to process\n",
    "    choice = \"1\"  # Default choice\n",
    "    \n",
    "    try:\n",
    "        # Try to get user input\n",
    "        import sys\n",
    "        if hasattr(sys, 'ps1') and len(corpus_files) > 1:  # Interactive session with multiple files\n",
    "            choice = input(f\"\\nChoose corpus file to process (1-{len(corpus_files)}) [default=1]: \").strip()\n",
    "        else:\n",
    "            print(f\"\\nâš¡ Auto-selecting first file for processing\")\n",
    "            choice = \"1\"\n",
    "    except (EOFError, KeyboardInterrupt):\n",
    "        print(f\"\\nâš¡ No input received, using first file\")\n",
    "        choice = \"1\"\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš¡ Input error ({e}), using first file\")\n",
    "        choice = \"1\"\n",
    "    \n",
    "    # Fallback to \"1\" if choice is empty\n",
    "    if not choice:\n",
    "        choice = \"1\"\n",
    "        print(\"âš¡ Empty input, using first file\")\n",
    "    \n",
    "    try:\n",
    "        file_index = int(choice) - 1\n",
    "        if 0 <= file_index < len(corpus_files):\n",
    "            selected_file = corpus_files[file_index]\n",
    "        else:\n",
    "            print(f\"âŒ Invalid selection. Using first file as fallback.\")\n",
    "            selected_file = corpus_files[0]\n",
    "    except ValueError:\n",
    "        print(f\"âŒ Invalid input. Using first file as fallback.\")\n",
    "        selected_file = corpus_files[0]\n",
    "\n",
    "# Display final selection\n",
    "if selected_file:\n",
    "    print(f\"\\nðŸŽ¯ Selected corpus file: {os.path.basename(selected_file)}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No corpus file selected for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a9204",
   "metadata": {},
   "source": [
    "## 3. Text Processing and Tokenization\n",
    "\n",
    "Process the selected corpus file to extract word frequencies:\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Load and Clean**: Read the corpus file and remove any metadata comments\n",
    "2. **Tokenize**: Extract words using regex pattern matching\n",
    "3. **Calculate Frequencies**: Count word frequencies for the corpus\n",
    "\n",
    "**Generated Variables:**\n",
    "- `llm_frequency_raw`: Raw frequency count from the selected corpus\n",
    "- Word length and other derived variables for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8f0d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing selected corpus file...\n",
      "\n",
      "ðŸ“– Processing: large_corpus_2m_qwen_dynamic_20250908_002429.txt\n",
      "   âœ… Loaded 15,517,265 characters\n",
      "   ðŸ“Š Total tokens: 2,105,954\n",
      "   ðŸ“š Unique words: 27,939\n",
      "   ðŸ† Top 5 words: [('the', 108943), ('a', 79787), ('of', 53158), ('and', 49441), ('it', 40292)]\n",
      "\n",
      "âœ… DataFrame created:\n",
      "   ðŸ“š Vocabulary size: 27,939 words\n",
      "   ðŸ“Š Frequency range: 1-108943\n",
      "\n",
      "ðŸ“‹ Preview of frequency data:\n",
      "   word  word_length  llm_frequency_raw\n",
      "0   the            3             108943\n",
      "1     a            1              79787\n",
      "2    of            2              53158\n",
      "3   and            3              49441\n",
      "4    it            2              40292\n",
      "5    to            2              35514\n",
      "6    in            2              35280\n",
      "7     s            1              31289\n",
      "8  that            4              25366\n",
      "9     i            1              23607\n",
      "   ðŸ“Š Total tokens: 2,105,954\n",
      "   ðŸ“š Unique words: 27,939\n",
      "   ðŸ† Top 5 words: [('the', 108943), ('a', 79787), ('of', 53158), ('and', 49441), ('it', 40292)]\n",
      "\n",
      "âœ… DataFrame created:\n",
      "   ðŸ“š Vocabulary size: 27,939 words\n",
      "   ðŸ“Š Frequency range: 1-108943\n",
      "\n",
      "ðŸ“‹ Preview of frequency data:\n",
      "   word  word_length  llm_frequency_raw\n",
      "0   the            3             108943\n",
      "1     a            1              79787\n",
      "2    of            2              53158\n",
      "3   and            3              49441\n",
      "4    it            2              40292\n",
      "5    to            2              35514\n",
      "6    in            2              35280\n",
      "7     s            1              31289\n",
      "8  that            4              25366\n",
      "9     i            1              23607\n"
     ]
    }
   ],
   "source": [
    "# Single Corpus Processing\n",
    "print(\"ðŸ”„ Processing selected corpus file...\")\n",
    "\n",
    "if not selected_file:\n",
    "    print(\"âŒ No corpus file selected for processing\")\n",
    "    df_words = pd.DataFrame()\n",
    "else:\n",
    "    try:\n",
    "        filename = os.path.basename(selected_file)\n",
    "        print(f\"\\nðŸ“– Processing: {filename}\")\n",
    "        \n",
    "        # Load corpus text\n",
    "        with open(selected_file, 'r', encoding='utf-8') as file:\n",
    "            corpus_text = file.read()\n",
    "        \n",
    "        print(f\"   âœ… Loaded {len(corpus_text):,} characters\")\n",
    "        \n",
    "        # Clean text (remove metadata comments)\n",
    "        cleaned_text = re.sub(r'<!-- Story Metadata:.*?-->', '', corpus_text, flags=re.DOTALL)\n",
    "        \n",
    "        # Tokenize (extract words)\n",
    "        words = re.findall(r\"\\b[a-z]+(?:'[a-z]+)?\\b\", cleaned_text.lower())\n",
    "        \n",
    "        # Calculate word frequencies\n",
    "        word_counts = Counter(words)\n",
    "        total_words = len(words)\n",
    "        unique_words = len(word_counts)\n",
    "        \n",
    "        print(f\"   ðŸ“Š Total tokens: {total_words:,}\")\n",
    "        print(f\"   ðŸ“š Unique words: {unique_words:,}\")\n",
    "        print(f\"   ðŸ† Top 5 words: {word_counts.most_common(5)}\")\n",
    "        \n",
    "        # Create DataFrame with standard column names\n",
    "        vocabulary = sorted(word_counts.keys())\n",
    "        df_words = pd.DataFrame({'word': vocabulary})\n",
    "        df_words['word_length'] = df_words['word'].apply(len)\n",
    "        df_words['llm_frequency_raw'] = df_words['word'].map(word_counts)\n",
    "        \n",
    "        # Sort by frequency\n",
    "        df_words = df_words.sort_values('llm_frequency_raw', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nâœ… DataFrame created:\")\n",
    "        print(f\"   ðŸ“š Vocabulary size: {len(df_words):,} words\")\n",
    "        print(f\"   ðŸ“Š Frequency range: {df_words['llm_frequency_raw'].min()}-{df_words['llm_frequency_raw'].max()}\")\n",
    "        \n",
    "        # Display preview\n",
    "        print(f\"\\nðŸ“‹ Preview of frequency data:\")\n",
    "        print(df_words[['word', 'word_length', 'llm_frequency_raw']].head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error processing {filename}: {e}\")\n",
    "        df_words = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69af00",
   "metadata": {},
   "source": [
    "## 4. Display Corpus Statistics\n",
    "\n",
    "Let's examine the characteristics of our corpus dataset to understand the vocabulary and frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Corpus Dataset Summary\n",
      "========================================\n",
      "ðŸ“š Total unique words: 27,939\n",
      "ðŸ“ Average word length: 7.7 characters\n",
      "ðŸ“Š Word length range: 1 - 23 characters\n",
      "ðŸ“Š Total tokens in corpus: 2,105,954\n",
      "ðŸ“Š Frequency range: 1 - 108943\n",
      "\n",
      "ðŸ“Š Frequency Distribution:\n",
      "   Median frequency: 4\n",
      "   75th percentile: 20\n",
      "   95th percentile: 181\n",
      "\n",
      "ðŸ† Top 10 most frequent words:\n",
      "   the: 108,943\n",
      "   a: 79,787\n",
      "   of: 53,158\n",
      "   and: 49,441\n",
      "   it: 40,292\n",
      "   to: 35,514\n",
      "   in: 35,280\n",
      "   s: 31,289\n",
      "   that: 25,366\n",
      "   i: 23,607\n",
      "\n",
      "âœ… Corpus dataset ready for integration!\n"
     ]
    }
   ],
   "source": [
    "# Display Corpus Statistics\n",
    "print(\"ðŸ“ˆ Corpus Dataset Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if df_words.empty:\n",
    "    print(\"âŒ No data available for analysis.\")\n",
    "else:\n",
    "    # Overall statistics\n",
    "    print(f\"ðŸ“š Total unique words: {len(df_words):,}\")\n",
    "    print(f\"ðŸ“ Average word length: {df_words['word_length'].mean():.1f} characters\")\n",
    "    print(f\"ðŸ“Š Word length range: {df_words['word_length'].min()} - {df_words['word_length'].max()} characters\")\n",
    "    print(f\"ðŸ“Š Total tokens in corpus: {df_words['llm_frequency_raw'].sum():,}\")\n",
    "    print(f\"ðŸ“Š Frequency range: {df_words['llm_frequency_raw'].min()} - {df_words['llm_frequency_raw'].max()}\")\n",
    "    \n",
    "    # Frequency distribution\n",
    "    print(f\"\\nðŸ“Š Frequency Distribution:\")\n",
    "    freq_stats = df_words['llm_frequency_raw'].describe()\n",
    "    print(f\"   Median frequency: {freq_stats['50%']:.0f}\")\n",
    "    print(f\"   75th percentile: {freq_stats['75%']:.0f}\")\n",
    "    print(f\"   95th percentile: {df_words['llm_frequency_raw'].quantile(0.95):.0f}\")\n",
    "    \n",
    "    # High frequency words\n",
    "    print(f\"\\nðŸ† Top 10 most frequent words:\")\n",
    "    top_words = df_words.head(10)[['word', 'llm_frequency_raw']]\n",
    "    for _, row in top_words.iterrows():\n",
    "        print(f\"   {row['word']}: {row['llm_frequency_raw']:,}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Corpus dataset ready for integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfbaff",
   "metadata": {},
   "source": [
    "## 5. Data Integration with External Datasets\n",
    "\n",
    "Merge our LLM-derived frequencies with established psycholinguistic datasets:\n",
    "\n",
    "1. **LLM frequency**: Our corpus-derived frequency measures\n",
    "2. **Reference measures**: ECP, SUBTLEX, Multilex, GPT familiarity ratings  \n",
    "3. **Behavioral data**: Human reading times for validation\n",
    "\n",
    "This creates a comprehensive dataset for comparing LLM-derived predictors against established measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Integrating corpus data with reference datasets...\n",
      "âœ… Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "âœ… Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "âœ… Loaded 74286 records from SUBTLEX-US dataset\n",
      "âœ… Found reading time column(s): ['rt_correct_mean', 'rt_correct_std']\n",
      "âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\n",
      "âœ… Successfully merged corpus data with reference measures\n",
      "\n",
      "ðŸ“Š Integration Statistics:\n",
      "   ðŸ“š Reference measures available:\n",
      "      â€¢ subtlex_zipf: 15,859 words (56.8%)\n",
      "      â€¢ subtlex_freq_raw: 14,229 words (50.9%)\n",
      "      â€¢ multilex_zipf: 15,814 words (56.6%)\n",
      "      â€¢ gpt_familiarity: 15,859 words (56.8%)\n",
      "      â€¢ rt: 15,859 words (56.8%)\n",
      "      â€¢ accuracy: 15,859 words (56.8%)\n",
      "      â€¢ prevalence: 15,859 words (56.8%)\n",
      "\n",
      "âœ… Reading time data successfully integrated!\n",
      "   Column 'rt' contains mean correct reaction times from ECP\n",
      "\n",
      "âœ… Final dataset: 27939 words with 12 columns\n",
      "âœ… Loaded 74286 records from SUBTLEX-US dataset\n",
      "âœ… Found reading time column(s): ['rt_correct_mean', 'rt_correct_std']\n",
      "âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\n",
      "âœ… Successfully merged corpus data with reference measures\n",
      "\n",
      "ðŸ“Š Integration Statistics:\n",
      "   ðŸ“š Reference measures available:\n",
      "      â€¢ subtlex_zipf: 15,859 words (56.8%)\n",
      "      â€¢ subtlex_freq_raw: 14,229 words (50.9%)\n",
      "      â€¢ multilex_zipf: 15,814 words (56.6%)\n",
      "      â€¢ gpt_familiarity: 15,859 words (56.8%)\n",
      "      â€¢ rt: 15,859 words (56.8%)\n",
      "      â€¢ accuracy: 15,859 words (56.8%)\n",
      "      â€¢ prevalence: 15,859 words (56.8%)\n",
      "\n",
      "âœ… Reading time data successfully integrated!\n",
      "   Column 'rt' contains mean correct reaction times from ECP\n",
      "\n",
      "âœ… Final dataset: 27939 words with 12 columns\n"
     ]
    }
   ],
   "source": [
    "# Data Integration with Reference Datasets\n",
    "print(\"ðŸ”— Integrating corpus data with reference datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load ECP data using path utilities\n",
    "    ecp_path = get_project_path('data/lexicaldecision/ecp/English Crowdsourcing Project All Native Speakers.csv')\n",
    "    ecp_df = pd.read_csv(ecp_path)\n",
    "    print(f\"âœ… Loaded {len(ecp_df)} records from ECP dataset\")\n",
    "    \n",
    "    # Load SUBTLEX-US data to get raw frequency counts\n",
    "    print(\"Loading SUBTLEX-US data for raw frequency counts...\")\n",
    "    subtlex_us_path = get_project_path('data/frequency/subtlex-us/SUBTLEXus74286wordstextversion.txt')\n",
    "    subtlex_df = pd.read_csv(subtlex_us_path, sep='\\t')\n",
    "    print(f\"âœ… Loaded {len(subtlex_df)} records from SUBTLEX-US dataset\")\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    subtlex_df = subtlex_df.rename(columns={\n",
    "        'Word': 'word',\n",
    "        'FREQcount': 'subtlex_freq_raw'\n",
    "    })\n",
    "    \n",
    "    # Merge ECP data with SUBTLEX-US data to get raw frequency counts\n",
    "    word_col = 'spelling' if 'spelling' in ecp_df.columns else 'Word'\n",
    "    ecp_with_subtlex = pd.merge(ecp_df, subtlex_df[['word', 'subtlex_freq_raw']], \n",
    "                               left_on=word_col, \n",
    "                               right_on='word', how='left')\n",
    "    \n",
    "    # Define reference columns to merge (using actual ECP column names!)\n",
    "    ref_cols = ['SUBTLEX', 'subtlex_freq_raw', 'Multilex', 'GPT', 'rt_correct_mean', 'accuracy', 'prevalence']\n",
    "    cols_to_merge = [word_col] + [col for col in ref_cols if col in ecp_with_subtlex.columns]\n",
    "    \n",
    "    # Check what reading time column is available\n",
    "    rt_cols = [col for col in ecp_with_subtlex.columns if 'rt' in col.lower() or 'reaction' in col.lower()]\n",
    "    if rt_cols:\n",
    "        print(f\"âœ… Found reading time column(s): {rt_cols}\")\n",
    "        cols_to_merge.extend([col for col in rt_cols if col not in cols_to_merge])\n",
    "    else:\n",
    "        print(\"âš ï¸ No reading time column found in ECP data\")\n",
    "        print(f\"   Available ECP columns: {list(ecp_with_subtlex.columns)[:10]}...\")\n",
    "    \n",
    "    # Merge corpus data with reference data\n",
    "    merged_df = pd.merge(df_words, ecp_with_subtlex[cols_to_merge], \n",
    "                        left_on='word', right_on=word_col, how='left')\n",
    "    \n",
    "    # Rename reference columns for clarity (using actual ECP column names)\n",
    "    column_renames = {\n",
    "        'SUBTLEX': 'subtlex_zipf',\n",
    "        'subtlex_freq_raw': 'subtlex_freq_raw',\n",
    "        'Multilex': 'multilex_zipf',\n",
    "        'GPT': 'gpt_familiarity',\n",
    "        'rt_correct_mean': 'rt',  # Rename ECP reading time to standard 'rt'\n",
    "        'spelling': 'word'  # Ensure word column is consistently named\n",
    "    }\n",
    "    \n",
    "    # Apply column renaming\n",
    "    merged_df = merged_df.rename(columns=column_renames)\n",
    "    print(f\"âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\")\n",
    "    \n",
    "    # Remove duplicate word column if present\n",
    "    if word_col != 'word' and word_col in merged_df.columns:\n",
    "        merged_df = merged_df.drop(columns=[word_col])\n",
    "        \n",
    "    print(\"âœ… Successfully merged corpus data with reference measures\")\n",
    "    \n",
    "    # Show integration statistics\n",
    "    ref_columns = ['subtlex_zipf', 'subtlex_freq_raw', 'multilex_zipf', 'gpt_familiarity', 'rt', 'accuracy', 'prevalence']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Integration Statistics:\")\n",
    "    print(f\"   ðŸ“š Reference measures available:\")\n",
    "    for col in ref_columns:\n",
    "        if col in merged_df.columns:\n",
    "            coverage = merged_df[col].notna().sum()\n",
    "            print(f\"      â€¢ {col}: {coverage:,} words ({coverage/len(merged_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"      â€¢ {col}: NOT AVAILABLE\")\n",
    "            \n",
    "    # Special check for reading time data\n",
    "    if 'rt' not in merged_df.columns:\n",
    "        print(f\"\\nâš ï¸ CRITICAL: No reading time (rt) column found!\")\n",
    "        print(f\"   This will cause issues in notebook2_corpus_analysis.ipynb\")\n",
    "        print(f\"   Please check the ECP data file for reading time columns.\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Reading time data successfully integrated!\")\n",
    "        print(f\"   Column 'rt' contains mean correct reaction times from ECP\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ Reference data file not found: {e}\")\n",
    "    print(\"   Proceeding with LLM data only.\")\n",
    "    merged_df = df_words.copy()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error during data integration: {e}\")\n",
    "    print(\"   Proceeding with LLM data only.\")\n",
    "    merged_df = df_words.copy()\n",
    "\n",
    "print(f\"\\nâœ… Final dataset: {len(merged_df)} words with {len(merged_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5379e",
   "metadata": {},
   "source": [
    "## 6. Data Integration with External Datasets\n",
    "\n",
    "**Integration Strategy:** We merge our single LLM-derived corpus frequencies with established psycholinguistic datasets to create a comprehensive comparative dataset:\n",
    "\n",
    "1. **LLM frequency measures**: Raw frequency counts from our selected corpus\n",
    "2. **Reference measures**: ECP, SUBTLEX, Multilex, GPT familiarity ratings\n",
    "3. **Behavioral data**: Human reading times for validation against real reading behavior\n",
    "\n",
    "**Generated Variables:**\n",
    "- `llm_frequency_raw`: Raw frequency count from the selected corpus\n",
    "- `subtlex_zipf`: SUBTLEX frequency in Zipf scale\n",
    "- `multilex_zipf`: Multilex frequency in Zipf scale  \n",
    "- `gpt_familiarity`: GPT-based familiarity estimates\n",
    "- `rt`: Human reading times from ECP dataset\n",
    "\n",
    "This creates a focused comparison between:\n",
    "- LLM-derived frequency vs. traditional corpus frequency (SUBTLEX)\n",
    "- LLM-derived frequency vs. LLM-based familiarity (GPT estimates)\n",
    "- All frequency measures vs. actual human reading behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Integrating multi-corpus data with reference datasets...\n",
      "âœ… Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "âœ… Loaded 74286 records from SUBTLEX-US dataset\n",
      "âœ… Loaded 61851 records from ECP dataset\n",
      "Loading SUBTLEX-US data for raw frequency counts...\n",
      "âœ… Loaded 74286 records from SUBTLEX-US dataset\n",
      "âœ… Found reading time column(s): ['rt_correct_mean', 'rt_correct_std']\n",
      "âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\n",
      "âœ… Successfully merged multi-corpus data with reference measures\n",
      "\n",
      "ðŸ“Š Integration Statistics:\n",
      "   ðŸ“š Reference measures available:\n",
      "      â€¢ subtlex_zipf: 15,859 words (56.8%)\n",
      "      â€¢ subtlex_freq_raw: 14,229 words (50.9%)\n",
      "      â€¢ multilex_zipf: 15,814 words (56.6%)\n",
      "      â€¢ gpt_familiarity: 15,859 words (56.8%)\n",
      "      â€¢ rt: 15,859 words (56.8%)\n",
      "      â€¢ accuracy: 15,859 words (56.8%)\n",
      "      â€¢ prevalence: 15,859 words (56.8%)\n",
      "\n",
      "âœ… Reading time data successfully integrated!\n",
      "   Column 'rt' contains mean correct reaction times from ECP\n",
      "\n",
      "âœ… Final dataset: 27939 words with 12 columns\n",
      "âœ… Found reading time column(s): ['rt_correct_mean', 'rt_correct_std']\n",
      "âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\n",
      "âœ… Successfully merged multi-corpus data with reference measures\n",
      "\n",
      "ðŸ“Š Integration Statistics:\n",
      "   ðŸ“š Reference measures available:\n",
      "      â€¢ subtlex_zipf: 15,859 words (56.8%)\n",
      "      â€¢ subtlex_freq_raw: 14,229 words (50.9%)\n",
      "      â€¢ multilex_zipf: 15,814 words (56.6%)\n",
      "      â€¢ gpt_familiarity: 15,859 words (56.8%)\n",
      "      â€¢ rt: 15,859 words (56.8%)\n",
      "      â€¢ accuracy: 15,859 words (56.8%)\n",
      "      â€¢ prevalence: 15,859 words (56.8%)\n",
      "\n",
      "âœ… Reading time data successfully integrated!\n",
      "   Column 'rt' contains mean correct reaction times from ECP\n",
      "\n",
      "âœ… Final dataset: 27939 words with 12 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced Multi-Corpus Data Integration ---\n",
    "print(\"ðŸ”— Integrating multi-corpus data with reference datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load ECP data using path utilities\n",
    "    ecp_path = get_project_path('data/lexicaldecision/ecp/English Crowdsourcing Project All Native Speakers.csv')\n",
    "    ecp_df = pd.read_csv(ecp_path)\n",
    "    print(f\"âœ… Loaded {len(ecp_df)} records from ECP dataset\")\n",
    "    \n",
    "    # Load SUBTLEX-US data to get raw frequency counts\n",
    "    print(\"Loading SUBTLEX-US data for raw frequency counts...\")\n",
    "    subtlex_us_path = get_project_path('data/frequency/subtlex-us/SUBTLEXus74286wordstextversion.txt')\n",
    "    subtlex_df = pd.read_csv(subtlex_us_path, sep='\\t')\n",
    "    print(f\"âœ… Loaded {len(subtlex_df)} records from SUBTLEX-US dataset\")\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    subtlex_df = subtlex_df.rename(columns={\n",
    "        'Word': 'word',\n",
    "        'FREQcount': 'subtlex_freq_raw'\n",
    "    })\n",
    "    \n",
    "    # Merge ECP data with SUBTLEX-US data to get raw frequency counts\n",
    "    word_col = 'spelling' if 'spelling' in ecp_df.columns else 'Word'\n",
    "    ecp_with_subtlex = pd.merge(ecp_df, subtlex_df[['word', 'subtlex_freq_raw']], \n",
    "                               left_on=word_col, \n",
    "                               right_on='word', how='left')\n",
    "    \n",
    "    # Define reference columns to merge (using actual ECP column names!)\n",
    "    ref_cols = ['SUBTLEX', 'subtlex_freq_raw', 'Multilex', 'GPT', 'rt_correct_mean', 'accuracy', 'prevalence']\n",
    "    cols_to_merge = [word_col] + [col for col in ref_cols if col in ecp_with_subtlex.columns]\n",
    "    \n",
    "    # Check what reading time column is available\n",
    "    rt_cols = [col for col in ecp_with_subtlex.columns if 'rt' in col.lower() or 'reaction' in col.lower()]\n",
    "    if rt_cols:\n",
    "        print(f\"âœ… Found reading time column(s): {rt_cols}\")\n",
    "        cols_to_merge.extend([col for col in rt_cols if col not in cols_to_merge])\n",
    "    else:\n",
    "        print(\"âš ï¸ No reading time column found in ECP data\")\n",
    "        print(f\"   Available ECP columns: {list(ecp_with_subtlex.columns)[:10]}...\")\n",
    "    \n",
    "    # Merge multi-corpus data with reference data\n",
    "    merged_df = pd.merge(df_words, ecp_with_subtlex[cols_to_merge], \n",
    "                        left_on='word', right_on=word_col, how='left')\n",
    "    \n",
    "    # Rename reference columns for clarity (using actual ECP column names)\n",
    "    column_renames = {\n",
    "        'SUBTLEX': 'subtlex_zipf',\n",
    "        'subtlex_freq_raw': 'subtlex_freq_raw',\n",
    "        'Multilex': 'multilex_zipf',\n",
    "        'GPT': 'gpt_familiarity',\n",
    "        'rt_correct_mean': 'rt',  # Rename ECP reading time to standard 'rt'\n",
    "        'spelling': 'word'  # Ensure word column is consistently named\n",
    "    }\n",
    "    \n",
    "    # Apply column renaming\n",
    "    merged_df = merged_df.rename(columns=column_renames)\n",
    "    print(f\"âœ… Reading time column 'rt_correct_mean' renamed to 'rt'\")\n",
    "    \n",
    "    # Remove duplicate word column if present\n",
    "    if word_col != 'word' and word_col in merged_df.columns:\n",
    "        merged_df = merged_df.drop(columns=[word_col])\n",
    "        \n",
    "    print(\"âœ… Successfully merged multi-corpus data with reference measures\")\n",
    "    \n",
    "    # Show integration statistics\n",
    "    ref_columns = ['subtlex_zipf', 'subtlex_freq_raw', 'multilex_zipf', 'gpt_familiarity', 'rt', 'accuracy', 'prevalence']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Integration Statistics:\")\n",
    "    print(f\"   ðŸ“š Reference measures available:\")\n",
    "    for col in ref_columns:\n",
    "        if col in merged_df.columns:\n",
    "            coverage = merged_df[col].notna().sum()\n",
    "            print(f\"      â€¢ {col}: {coverage:,} words ({coverage/len(merged_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"      â€¢ {col}: NOT AVAILABLE\")\n",
    "            \n",
    "    # Special check for reading time data\n",
    "    if 'rt' not in merged_df.columns:\n",
    "        print(f\"\\nâš ï¸ CRITICAL: No reading time (rt) column found!\")\n",
    "        print(f\"   This will cause issues in notebook2_corpus_analysis.ipynb\")\n",
    "        print(f\"   Please check the ECP data file for reading time columns.\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Reading time data successfully integrated!\")\n",
    "        print(f\"   Column 'rt' contains mean correct reaction times from ECP\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ Reference data file not found: {e}\")\n",
    "    print(\"   Proceeding with LLM data only.\")\n",
    "    merged_df = df_words.copy()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error during data integration: {e}\")\n",
    "    print(\"   Proceeding with LLM data only.\")\n",
    "    merged_df = df_words.copy()\n",
    "\n",
    "print(f\"\\nâœ… Final dataset: {len(merged_df)} words with {len(merged_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be143e7c",
   "metadata": {},
   "source": [
    "## 7. Frequency Transformations\n",
    "\n",
    "**Transformation Strategy:** We apply logarithmic transformations to prepare the frequency data for regression analysis.\n",
    "\n",
    "**For the corpus, we create:**\n",
    "1. **Schepens Transformation**: `log10(frequency + 1)` - Simple log transformation used in Schepens et al. paper\n",
    "2. **Zipf Transformation**: `log10((frequency / total_tokens) * 1e9)` - Standard Zipf-scale frequency\n",
    "\n",
    "**Resulting variables:**\n",
    "- `llm_frequency_schepens` - Log-transformed frequency following Schepens method\n",
    "- `llm_frequency_zipf` - Zipf-scale frequency for comparison with reference measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "g8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Computing frequency transformations...\n",
      "âœ… Added log10(frequency + 1) transformation (Schepens method)\n",
      "âœ… Added Zipf-scale frequency transformation\n",
      "\n",
      "ðŸ“Š Frequency measure statistics:\n",
      "\n",
      "ðŸ“ llm_frequency_raw:\n",
      "   Valid values: 27,939 (100.0%)\n",
      "   Range: 1.000 to 108943.000\n",
      "   Mean: 75.377\n",
      "   Median: 4.000\n",
      "\n",
      "ðŸ“ llm_frequency_schepens:\n",
      "   Valid values: 27,939 (100.0%)\n",
      "   Range: 0.301 to 5.037\n",
      "   Mean: 0.932\n",
      "   Median: 0.699\n",
      "\n",
      "ðŸ“ llm_frequency_zipf:\n",
      "   Valid values: 27,939 (100.0%)\n",
      "   Range: 2.677 to 7.714\n",
      "   Mean: 3.478\n",
      "   Median: 3.279\n",
      "\n",
      "ðŸ“ subtlex_zipf:\n",
      "   Valid values: 15,859 (56.8%)\n",
      "   Range: 1.292 to 7.621\n",
      "   Mean: 3.193\n",
      "   Median: 3.137\n",
      "\n",
      "ðŸ“ multilex_zipf:\n",
      "   Valid values: 15,814 (56.6%)\n",
      "   Range: 0.374 to 7.572\n",
      "   Mean: 3.371\n",
      "   Median: 3.331\n",
      "\n",
      "ðŸ“ gpt_familiarity:\n",
      "   Valid values: 15,859 (56.8%)\n",
      "   Range: 1.008 to 7.000\n",
      "   Mean: 5.841\n",
      "   Median: 6.023\n",
      "\n",
      "âœ… Frequency transformations complete!\n",
      "   Final dataset: 27939 words with 14 measures\n"
     ]
    }
   ],
   "source": [
    "# Frequency Transformations for Regression Analysis\n",
    "print(\"ðŸ”¢ Computing frequency transformations...\")\n",
    "\n",
    "# Calculate Schepens-style log-transformed frequency\n",
    "if 'llm_frequency_raw' in merged_df.columns:\n",
    "    # Ensure positive values and add small constant to avoid log(0)\n",
    "    merged_df['llm_frequency_schepens'] = np.log10(merged_df['llm_frequency_raw'] + 1)\n",
    "    print(\"âœ… Added log10(frequency + 1) transformation (Schepens method)\")\n",
    "    \n",
    "    # Calculate Zipf-scale frequency (log10 per billion)\n",
    "    total_tokens = merged_df['llm_frequency_raw'].sum()\n",
    "    if total_tokens > 0:\n",
    "        merged_df['llm_frequency_zipf'] = np.log10((merged_df['llm_frequency_raw'] / total_tokens) * 1e9)\n",
    "        # Replace -inf with minimum valid value for zero-frequency words\n",
    "        min_valid_zipf = merged_df.loc[merged_df['llm_frequency_zipf'] != -np.inf, 'llm_frequency_zipf'].min()\n",
    "        merged_df.loc[merged_df['llm_frequency_zipf'] == -np.inf, 'llm_frequency_zipf'] = min_valid_zipf - 1\n",
    "        print(\"âœ… Added Zipf-scale frequency transformation\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Cannot calculate Zipf frequency: total tokens is 0\")\n",
    "        merged_df['llm_frequency_zipf'] = np.nan\n",
    "else:\n",
    "    print(\"âš ï¸ No LLM frequency data found for transformations\")\n",
    "\n",
    "# Verify and display transformation results\n",
    "numeric_cols = ['llm_frequency_raw', 'llm_frequency_schepens', 'llm_frequency_zipf', \n",
    "               'subtlex_zipf', 'multilex_zipf', 'gpt_familiarity']\n",
    "\n",
    "available_cols = [col for col in numeric_cols if col in merged_df.columns]\n",
    "\n",
    "if available_cols:\n",
    "    print(f\"\\nðŸ“Š Frequency measure statistics:\")\n",
    "    \n",
    "    for col in available_cols:\n",
    "        valid_count = merged_df[col].notna().sum()\n",
    "        print(f\"\\nðŸ“ {col}:\")\n",
    "        print(f\"   Valid values: {valid_count:,} ({valid_count/len(merged_df)*100:.1f}%)\")\n",
    "        if valid_count > 0:\n",
    "            print(f\"   Range: {merged_df[col].min():.3f} to {merged_df[col].max():.3f}\")\n",
    "            print(f\"   Mean: {merged_df[col].mean():.3f}\")\n",
    "            print(f\"   Median: {merged_df[col].median():.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… Frequency transformations complete!\")\n",
    "print(f\"   Final dataset: {len(merged_df)} words with {len(merged_df.columns)} measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0896c6f",
   "metadata": {},
   "source": [
    "## 8. Export Dataset for Analysis\n",
    "\n",
    "**Final Output:** We export the complete dataset with all frequency measures and reference data for analysis in `notebook2_corpus_analysis.ipynb`.\n",
    "\n",
    "**The exported file includes:**\n",
    "- **LLM Frequency Measures**: Raw frequency, Schepens-transformed, and Zipf-scaled\n",
    "- **Reference Measures**: SUBTLEX, Multilex, GPT familiarity estimates\n",
    "- **Behavioral Data**: English reading times from ECP dataset\n",
    "- **Metadata**: Word length, basic statistics\n",
    "\n",
    "**Output:** `generated_corpus_with_predictors.csv` - ready for regression analysis and validation against human reading time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "h8e8f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to export corpus dataset...\n",
      "Dataset Summary:\n",
      "   Total words: 27,939\n",
      "   Total columns: 14\n",
      "\n",
      "Column Categories:\n",
      "   LLM Frequency Measures: 3 columns\n",
      "      - llm_frequency_raw\n",
      "      - llm_frequency_schepens\n",
      "      - llm_frequency_zipf\n",
      "   Reference Measures: 7 columns\n",
      "      - subtlex_zipf\n",
      "      - subtlex_freq_raw\n",
      "      - multilex_zipf\n",
      "      - gpt_familiarity\n",
      "      - rt\n",
      "      - accuracy\n",
      "      - prevalence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus dataset exported successfully!\n",
      "File: generated_corpus_with_predictors.csv\n",
      "File size: 3.5 MB\n",
      "\n",
      "Next Steps:\n",
      "   1. Open notebook2_corpus_analysis.ipynb for behavioral validation\n",
      "   2. Use this file to compare:\n",
      "      - LLM-derived frequency vs traditional measures (SUBTLEX, Multilex)\n",
      "      - LLM frequency vs GPT familiarity estimates\n",
      "      - All measures vs human reading times (ECP data)\n",
      "\n",
      "Available for Analysis:\n",
      "   10 frequency/familiarity predictors\n",
      "   Ready for statistical modeling and validation\n",
      "   Compatible with notebook2_corpus_analysis.ipynb regression analysis\n"
     ]
    }
   ],
   "source": [
    "# Export Dataset for Analysis\n",
    "print(\"Preparing to export corpus dataset...\")\n",
    "\n",
    "# Generate summary of available columns\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"   Total words: {len(merged_df):,}\")\n",
    "print(f\"   Total columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# Categorize columns\n",
    "llm_freq_cols = [col for col in merged_df.columns if col.startswith('llm_frequency')]\n",
    "ref_cols = [col for col in merged_df.columns if col in ['subtlex_zipf', 'subtlex_freq_raw', 'multilex_zipf', 'gpt_familiarity', 'rt', 'accuracy', 'prevalence']]\n",
    "\n",
    "print(f\"\\nColumn Categories:\")\n",
    "print(f\"   LLM Frequency Measures: {len(llm_freq_cols)} columns\")\n",
    "for col in llm_freq_cols:\n",
    "    print(f\"      - {col}\")\n",
    "    \n",
    "print(f\"   Reference Measures: {len(ref_cols)} columns\")\n",
    "for col in ref_cols:\n",
    "    if col in merged_df.columns:\n",
    "        print(f\"      - {col}\")\n",
    "\n",
    "# Export the dataset\n",
    "output_dir = get_project_path('output')\n",
    "output_path = os.path.join(output_dir, 'generated_corpus_with_predictors.csv')\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "# File size\n",
    "size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nCorpus dataset exported successfully!\")\n",
    "print(f\"File: {os.path.basename(output_path)}\")\n",
    "print(f\"File size: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   1. Open notebook2_corpus_analysis.ipynb for behavioral validation\")\n",
    "print(f\"   2. Use this file to compare:\")\n",
    "print(f\"      - LLM-derived frequency vs traditional measures (SUBTLEX, Multilex)\")\n",
    "print(f\"      - LLM frequency vs GPT familiarity estimates\")\n",
    "print(f\"      - All measures vs human reading times (ECP data)\")\n",
    "\n",
    "print(f\"\\nAvailable for Analysis:\")\n",
    "total_predictors = len(llm_freq_cols) + len(ref_cols)\n",
    "print(f\"   {total_predictors} frequency/familiarity predictors\")\n",
    "print(f\"   Ready for statistical modeling and validation\")\n",
    "print(f\"   Compatible with notebook2_corpus_analysis.ipynb regression analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6902f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
