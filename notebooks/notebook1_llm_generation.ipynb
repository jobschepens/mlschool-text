{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f9e5e3",
   "metadata": {},
   "source": [
    "# 🚀 Quick Setup for Colab/Binder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb177f",
   "metadata": {},
   "source": [
    "## 📚 Example: What We'll Build\n",
    "\n",
    "**Quick Preview**: This notebook will teach you to generate diverse text corpora using Large Language Models (LLMs) for psycholinguistic research.\n",
    "\n",
    "**Example Output**: By the end of this session, you'll have generated text like this:\n",
    "\n",
    "> *\"The researcher examined the cognitive mechanisms underlying language comprehension, focusing on how semantic processing influences reading speed. Meanwhile, in another domain, the chef carefully balanced flavors in the experimental fusion dish, combining traditional techniques with innovative molecular gastronomy approaches.\"*\n",
    "\n",
    "**What makes this valuable for research?**\n",
    "- **Vocabulary Diversity**: Mix of technical (cognitive, semantic) and everyday (chef, flavors) words\n",
    "- **Genre Variety**: Academic research + culinary arts in one corpus  \n",
    "- **Natural Frequency Patterns**: Common words (the, in) appear frequently, specialized terms (gastronomy) appear rarely\n",
    "- **Linguistic Complexity**: Varied sentence structures and lengths\n",
    "\n",
    "**Research Goal**: Create word frequency predictors from LLM-generated text that can predict human reading times as accurately as traditional corpus-based measures.\n",
    "\n",
    "---\n",
    "\n",
    "🎯 **Learning Path**: Setup → Interactive Generation → Large-Scale Corpus → Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcb2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 Local environment detected\n",
      "Make sure you've run: pip install -r requirements.txt\n",
      "📁 Working directory: c:\\GitHub\\mlschool-text\n",
      "✅ All key project files accessible\n",
      "🎯 Ready to start! You can now run the rest of the notebook.\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup (Run this first on Colab/Binder)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🔧 Setting up Google Colab environment...\")\n",
    "    # Clone the repository if not already present\n",
    "    if not os.path.exists('mlschool-text'):\n",
    "        !git clone https://github.com/jobschepens/mlschool-text.git\n",
    "        os.chdir('mlschool-text')\n",
    "    else:\n",
    "        os.chdir('mlschool-text')\n",
    "    \n",
    "    # Install requirements\n",
    "    !pip install -q -r requirements_colab.txt\n",
    "    print(\"✅ Colab setup complete!\")\n",
    "\n",
    "elif 'BINDER_LAUNCH_HOST' in os.environ:\n",
    "    print(\"🔧 Binder environment detected - dependencies should already be installed\")\n",
    "    print(\"✅ Binder setup complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"💻 Local environment detected\")\n",
    "    print(\"Make sure you've run: pip install -r requirements.txt\")\n",
    "\n",
    "# Set working directory for consistent paths\n",
    "if os.path.exists('mlschool-text') and not os.getcwd().endswith('mlschool-text'):\n",
    "    os.chdir('mlschool-text')\n",
    "elif os.getcwd().endswith('notebooks'):\n",
    "    # If we're in the notebooks directory, go up one level\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"📁 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify key files are accessible\n",
    "key_files = ['models.json', 'data/', 'scripts/', 'output/']\n",
    "missing_files = []\n",
    "for file_path in key_files:\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"⚠️ Warning: Cannot find {missing_files}\")\n",
    "    print(\"💡 Make sure you're in the correct directory\")\n",
    "else:\n",
    "    print(\"✅ All key project files accessible\")\n",
    "\n",
    "print(\"🎯 Ready to start! You can now run the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6677eda",
   "metadata": {},
   "source": [
    "## 🔐 API Key Setup for Google Colab & Binder\n",
    "\n",
    "**Important for cloud users**: Different platforms have different security considerations for API keys.\n",
    "\n",
    "### For Google Colab Users (Recommended)\n",
    "1. **Get an OpenRouter API key**: \n",
    "   - Go to [OpenRouter.ai](https://openrouter.ai) → Sign up → Get API key\n",
    "   - Free tier available with generous limits!\n",
    "\n",
    "2. **Store it securely in Colab**:\n",
    "   - In Colab, click the **🔑 key icon** on the left sidebar (Secrets)\n",
    "   - Click **\"Add new secret\"**\n",
    "   - Name: `OPENROUTER_API_KEY`\n",
    "   - Value: Paste your API key\n",
    "   - Toggle **\"Notebook access\"** to ON\n",
    "   - Click **\"Save\"**\n",
    "\n",
    "### For Binder Users\n",
    "Since Binder is ephemeral and public, you have three options:\n",
    "\n",
    "1. **🔐 Enter API key manually** - Secure but temporary (key disappears when session ends)\n",
    "2. **🎭 Demo mode** - Use realistic pre-generated examples (no API key needed)\n",
    "3. **📊 Analysis-only mode** - Skip generation entirely, just analyze pre-existing data\n",
    "\n",
    "### For Local Users\n",
    "Create a `.env` file in the project root with:\n",
    "```\n",
    "OPENROUTER_API_KEY=your_key_here\n",
    "```\n",
    "\n",
    "### Step 2: Run the setup cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce798c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded from .env file!\n",
      "🔑 API key configured (ends with: ...151c)\n"
     ]
    }
   ],
   "source": [
    "# Secure API Key Setup for Different Environments\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_BINDER = 'BINDER_LAUNCH_HOST' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use Colab's secure userdata feature\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('OPENROUTER_API_KEY')\n",
    "        os.environ['OPENROUTER_API_KEY'] = api_key\n",
    "        print(\"✅ API key loaded securely from Colab userdata!\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Could not load API key from Colab userdata.\")\n",
    "        print(\"Make sure you've added 'OPENROUTER_API_KEY' to your Colab secrets!\")\n",
    "        print(\"Instructions: Click 🔑 in left sidebar → Add new secret → Name: OPENROUTER_API_KEY\")\n",
    "        \n",
    "elif IN_BINDER:\n",
    "    print(\"🚀 Binder environment detected!\")\n",
    "    print()\n",
    "    print(\"📋 Options for API key in Binder:\")\n",
    "    print(\"1️⃣ Enter key manually (secure - not saved)\")\n",
    "    print(\"2️⃣ Use demo mode with pre-generated examples\") \n",
    "    print(\"3️⃣ Skip generation and analyze existing data\")\n",
    "    print()\n",
    "    \n",
    "    choice = input(\"Choose option (1, 2, or 3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        print(\"🔐 Enter your OpenRouter API key (input will be hidden):\")\n",
    "        api_key = getpass.getpass(\"API Key: \")\n",
    "        if api_key.strip():\n",
    "            os.environ['OPENROUTER_API_KEY'] = api_key.strip()\n",
    "            print(\"✅ API key set temporarily for this session!\")\n",
    "            print(\"🔒 Key will be cleared when Binder session ends\")\n",
    "        else:\n",
    "            print(\"❌ No API key entered\")\n",
    "    elif choice == \"2\":\n",
    "        print(\"🎭 Demo mode selected - will use pre-generated examples\")\n",
    "        os.environ['DEMO_MODE'] = 'true'\n",
    "        print(\"✅ Demo mode activated!\")\n",
    "    elif choice == \"3\":\n",
    "        print(\"📊 Analysis mode selected - will skip generation\")\n",
    "        os.environ['ANALYSIS_ONLY'] = 'true'\n",
    "        print(\"✅ Analysis-only mode activated!\")\n",
    "    else:\n",
    "        print(\"❓ Invalid choice - defaulting to demo mode\")\n",
    "        os.environ['DEMO_MODE'] = 'true'\n",
    "        \n",
    "else:\n",
    "    # Local environment - use .env file\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "    except ImportError:\n",
    "        pass  # dotenv not available, that's fine\n",
    "    \n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if api_key:\n",
    "        print(\"✅ API key loaded from .env file!\")\n",
    "    else:\n",
    "        print(\"❌ Please create a .env file with OPENROUTER_API_KEY=your_key_here\")\n",
    "        print(\"💡 Or set the environment variable directly\")\n",
    "\n",
    "# Verify the setup\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(f\"🔑 API key configured (ends with: ...{os.getenv('OPENROUTER_API_KEY')[-4:]})\")\n",
    "elif os.getenv(\"DEMO_MODE\"):\n",
    "    print(\"🎭 Running in demo mode - will use example responses\")\n",
    "elif os.getenv(\"ANALYSIS_ONLY\"):\n",
    "    print(\"📊 Running in analysis-only mode - will skip text generation\")\n",
    "else:\n",
    "    print(\"⚠️ No API key or mode selected - some features may not work\")\n",
    "    print(\"💡 Consider using demo mode to explore the concepts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce798c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 File accessibility check:\n",
      "   ✅ Model configuration file: models.json\n",
      "   ✅ Data directory: data/\n",
      "   ✅ Scripts directory: scripts/\n",
      "   ✅ Output directory: output/\n",
      "🎯 All project files accessible!\n",
      "💾 Output files will be saved to: output\n"
     ]
    }
   ],
   "source": [
    "# Path Utilities for Cross-Platform Compatibility\n",
    "import os\n",
    "\n",
    "def get_project_path(relative_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the correct path to project files regardless of working directory.\n",
    "    \n",
    "    Args:\n",
    "        relative_path (str): Path relative to project root (e.g., 'models.json', 'data/file.csv')\n",
    "    \n",
    "    Returns:\n",
    "        str: Absolute path to the file\n",
    "    \"\"\"\n",
    "    # If we're already in the project root, use the path directly\n",
    "    if os.path.exists(relative_path):\n",
    "        return relative_path\n",
    "    \n",
    "    # If we're in notebooks/ subdirectory, go up one level\n",
    "    parent_path = os.path.join('..', relative_path)\n",
    "    if os.path.exists(parent_path):\n",
    "        return parent_path\n",
    "    \n",
    "    # If we're in a cloned repo (like in Colab), try mlschool-text/ prefix\n",
    "    colab_path = os.path.join('mlschool-text', relative_path)\n",
    "    if os.path.exists(colab_path):\n",
    "        return colab_path\n",
    "    \n",
    "    # Fallback to original path (will give clear error if file doesn't exist)\n",
    "    return relative_path\n",
    "\n",
    "def get_output_path(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the correct path for output files (creates directory if needed).\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Output filename (e.g., 'corpus.txt', 'predictors.csv')\n",
    "    \n",
    "    Returns:\n",
    "        str: Full path to output file\n",
    "    \"\"\"\n",
    "    output_dir = get_project_path('output')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    return os.path.join(output_dir, filename)\n",
    "\n",
    "def check_file_access():\n",
    "    \"\"\"Check if key project files are accessible and show their paths.\"\"\"\n",
    "    key_files = {\n",
    "        'models.json': 'Model configuration file',\n",
    "        'data/': 'Data directory', \n",
    "        'scripts/': 'Scripts directory',\n",
    "        'output/': 'Output directory'\n",
    "    }\n",
    "    \n",
    "    print(\"📁 File accessibility check:\")\n",
    "    all_good = True\n",
    "    \n",
    "    for file_path, description in key_files.items():\n",
    "        full_path = get_project_path(file_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ✅ {description}: {full_path}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {description}: {full_path} (not found)\")\n",
    "            all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Run the check\n",
    "if check_file_access():\n",
    "    print(\"🎯 All project files accessible!\")\n",
    "    print(f\"💾 Output files will be saved to: {get_project_path('output')}\")\n",
    "else:\n",
    "    print(\"⚠️ Some files are missing - check your working directory\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(\"💡 Try re-running the environment setup cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f018c1f",
   "metadata": {},
   "source": [
    "# Part 1: Generating a Diverse LLM Corpus for Frequency Analysis\n",
    "## Session 1: From LLM Generation to Word Frequency\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Generate a diverse text corpus using an LLM to mirror the vocabulary found in large-scale datasets like the English Crowdsourcing Project (ECP).\n",
    "- Understand how prompt engineering across different genres (news, technical, fiction) can create a representative word frequency list.\n",
    "- Calculate word frequencies from the generated text to create a custom `llm_frequency` predictor.\n",
    "- Export the frequency data for comparative analysis in the next session.\n",
    "\n",
    "**Session Structure:**\n",
    "- **Setup & API Configuration** (10 minutes)\n",
    "- **Diverse Corpus Generation** (25 minutes)\n",
    "- **Frequency Calculation & Export** (10 minutes)\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Research Context:** Our goal is to create a high-quality `llm_frequency` predictor. The ECP dataset, which we use for validation in Notebook 2, contains a wide vocabulary from many sources (general use, dictionaries, etc.). To create a comparable predictor, we must generate a corpus that is equally diverse. A simple corpus (e.g., only children's stories) is insufficient. This session focuses on generating a varied corpus to capture a broad slice of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec55856",
   "metadata": {},
   "source": [
    "## 1.1 Setup and API Configuration\n",
    "\n",
    "Let's set up our environment and configure the API client to generate our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f31d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LLM Text Generation Session\n",
      "===================================\n",
      "Setting up environment for corpus generation...\n",
      "✅ Base environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and API Configuration\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "print(\"🚀 LLM Text Generation Session\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Setting up environment for corpus generation...\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✅ Base environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017d73a",
   "metadata": {},
   "source": [
    "\n",
    "Let's set up our environment and configure the API client to generate our corpus.\n",
    "\n",
    "First, we need a way to communicate with a Large Language Model (LLM). Instead of a complex setup, we'll use a single function that calls the **OpenRouter API**. OpenRouter allows us to access many different models (like GPT, Claude, Llama, etc.) through one consistent interface.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **API Key**: If you followed the setup instructions above, your API key should already be configured\n",
    "2. **Function Definition**: We define `call_openrouter`, which handles the API request  \n",
    "3. **Testing**: We'll run a quick test to ensure everything is configured correctly\n",
    "\n",
    "*If the test fails, please check the API key setup instructions in the cells above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdab6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing OpenRouter API function...\n",
      "✅ API Test Successful!\n",
      "   Prompt: Write a single sentence about psycholinguistics.\n",
      "   Response: Psycholinguistics is the study of how language is processed and understood by the human brain.\n",
      "   Response length: 94 characters\n",
      "   Mode: 🌐 Live API mode active\n"
     ]
    }
   ],
   "source": [
    "# Enhanced API Function with Demo Mode Support\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "\n",
    "def call_openrouter(prompt: str, model_name: str = \"openai/gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Calls the OpenRouter API with a specified model and prompt, or returns demo responses.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the LLM.\n",
    "        model_name (str): The model to use (e.g., \"openai/gpt-3.5-turbo\").\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text content from the LLM or demo response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for demo mode\n",
    "    if os.getenv(\"DEMO_MODE\"):\n",
    "        return get_demo_response(prompt)\n",
    "    \n",
    "    # Check for analysis-only mode\n",
    "    if os.getenv(\"ANALYSIS_ONLY\"):\n",
    "        return \"Analysis-only mode: Text generation skipped. Using pre-existing data for analysis.\"\n",
    "    \n",
    "    # Regular API call\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        error_msg = \"\"\"\n",
    "        ❌ OPENROUTER_API_KEY not found!\n",
    "        \n",
    "        📋 Setup Instructions:\n",
    "        🔹 Colab: Add key to Secrets (🔑 icon) → Run setup cell above\n",
    "        🔹 Local: Create .env file with OPENROUTER_API_KEY=your_key_here\n",
    "        🔹 Binder: Re-run setup cell and choose option 1, 2, or 3\n",
    "        \"\"\"\n",
    "        raise ValueError(error_msg)\n",
    "        \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json',\n",
    "        \"HTTP-Referer\": \"http://localhost:3000\",\n",
    "        \"X-Title\": \"ML-School-Psycholinguistics\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        'model': model_name,\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'max_tokens': 400,\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        if 'choices' in result and len(result['choices']) > 0:\n",
    "            return result['choices'][0]['message']['content'].strip()\n",
    "        else:\n",
    "            return f\"Error: Unexpected response format - {result}\"\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ API request failed: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ Response format error: {e}\")\n",
    "        return f\"Error: Unexpected response format\"\n",
    "\n",
    "def get_demo_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns realistic demo responses for different types of prompts.\n",
    "    \"\"\"\n",
    "    demo_responses = {\n",
    "        \"psycholinguistics\": [\n",
    "            \"Psycholinguistics is the study of how language is processed in the brain, combining insights from psychology, linguistics, and neuroscience.\",\n",
    "            \"The field of psycholinguistics examines how humans acquire, comprehend, and produce language through cognitive processes.\",\n",
    "            \"Researchers in psycholinguistics investigate the mental mechanisms underlying language comprehension and production.\"\n",
    "        ],\n",
    "        \"science\": [\n",
    "            \"The process of photosynthesis allows plants to convert sunlight into chemical energy, producing oxygen as a byproduct that sustains most life on Earth.\",\n",
    "            \"Gravity is a fundamental force that shapes the structure of the universe, from holding planets in orbit to influencing the formation of galaxies.\",\n",
    "            \"DNA contains the genetic instructions that guide the development and functioning of all living organisms.\"\n",
    "        ],\n",
    "        \"story\": [\n",
    "            \"Sarah discovered an old journal in her grandmother's attic, filled with pressed flowers and mysterious entries about a hidden garden that seemed to change with the seasons.\",\n",
    "            \"The lighthouse keeper noticed something unusual in the fog that night - a ship that appeared to be sailing backwards through time.\",\n",
    "            \"When Maya opened the peculiar music box, she found herself transported to a world where colors had sounds and melodies had taste.\"\n",
    "        ],\n",
    "        \"technical\": [\n",
    "            \"To optimize database performance, consider implementing proper indexing strategies, query optimization techniques, and regular maintenance procedures.\",\n",
    "            \"Machine learning algorithms can be broadly categorized into supervised, unsupervised, and reinforcement learning approaches, each suitable for different types of problems.\",\n",
    "            \"The software development lifecycle includes planning, analysis, design, implementation, testing, deployment, and maintenance phases.\"\n",
    "        ],\n",
    "        \"general\": [\n",
    "            \"Learning a new skill requires patience, practice, and persistence, but the rewards of mastering something challenging are immense.\",\n",
    "            \"Climate change represents one of the most significant challenges of our time, requiring coordinated global action and innovative solutions.\",\n",
    "            \"The rapid advancement of technology continues to transform how we work, communicate, and understand the world around us.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Choose appropriate demo responses based on prompt content\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"psycholinguist\" in prompt_lower or \"language\" in prompt_lower:\n",
    "        responses = demo_responses[\"psycholinguistics\"]\n",
    "    elif \"story\" in prompt_lower or \"character\" in prompt_lower or \"creative\" in prompt_lower:\n",
    "        responses = demo_responses[\"story\"]\n",
    "    elif \"technical\" in prompt_lower or \"how to\" in prompt_lower or \"algorithm\" in prompt_lower:\n",
    "        responses = demo_responses[\"technical\"]\n",
    "    elif \"science\" in prompt_lower or \"explain\" in prompt_lower:\n",
    "        responses = demo_responses[\"science\"]\n",
    "    else:\n",
    "        responses = demo_responses[\"general\"]\n",
    "    \n",
    "    return f\"[DEMO] {random.choice(responses)}\"\n",
    "\n",
    "# --- Test the API function ---\n",
    "print(\"🚀 Testing OpenRouter API function...\")\n",
    "try:\n",
    "    test_prompt = \"Write a single sentence about psycholinguistics.\"\n",
    "    test_response = call_openrouter(test_prompt)\n",
    "    print(f\"✅ API Test Successful!\")\n",
    "    print(f\"   Prompt: {test_prompt}\")\n",
    "    print(f\"   Response: {test_response}\")\n",
    "    print(f\"   Response length: {len(test_response)} characters\")\n",
    "    \n",
    "    # Show mode information\n",
    "    if os.getenv(\"DEMO_MODE\"):\n",
    "        print(\"   Mode: 🎭 Demo mode active\")\n",
    "    elif os.getenv(\"ANALYSIS_ONLY\"):\n",
    "        print(\"   Mode: 📊 Analysis-only mode active\")\n",
    "    else:\n",
    "        print(\"   Mode: 🌐 Live API mode active\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred during the test: {e}\")\n",
    "    print(\"💡 Try running the API key setup cell above first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fc8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8930f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Setup Complete!\n",
    "\n",
    "If the test above was successful, you're ready to proceed! Your environment now has:\n",
    "- ✅ All required libraries installed\n",
    "- ✅ API key configured securely  \n",
    "- ✅ OpenRouter connection verified\n",
    "- ✅ Ready for corpus generation\n",
    "\n",
    "**Next:** Continue to the interactive text generation section below to start exploring different prompts and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982c160",
   "metadata": {},
   "source": [
    "## 1.2 Interactive Text Generation \n",
    "Now for the hands-on part. We will generate short text samples using different models and prompts to understand how these choices influence the resulting text. This is the core of corpus generation methodology.\n",
    "\n",
    "**Our goals here are to:**\n",
    "- **Explore Model Differences**: See how different models (e.g., a creative one vs. a technical one) respond to the same prompt.\n",
    "- **Understand Prompt Engineering**: Learn how small changes to a prompt can alter the style, vocabulary, and complexity of the output.\n",
    "- **Observe Corpus Characteristics**: Get a feel for the kind of text each strategy produces.\n",
    "\n",
    "We'll start by defining a few models available through OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42aaaf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and selected 2 models from 'models.json'.\n",
      "\n",
      "🎯 Selected models for testing:\n",
      "   • Meta Llama 3.3 8B Instruct: meta-llama/llama-3.3-8b-instruct:free\n",
      "   • DeepSeek-V3.1: deepseek/deepseek-chat-v3.1:free\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Prompt Experimentation ---\n",
    "import json\n",
    "\n",
    "# Define the specific models we want to test in this notebook.\n",
    "# We will filter the larger list from models.json using these IDs.\n",
    "selected_model_ids = {\n",
    "    # \"qwen/qwen3-30b-a3b-instruct-2507\",\n",
    "    \"meta-llama/llama-3.3-8b-instruct:free\", # Note: Using the free tier ID from the JSON\n",
    "    \"deepseek/deepseek-chat-v3.1:free\"      # Note: Using the free tier ID from the JSON\n",
    "}\n",
    "\n",
    "# Load models from the JSON file using our path utility\n",
    "try:\n",
    "    models_path = get_project_path('models.json')\n",
    "    with open(models_path, 'r') as f:\n",
    "        models_data = json.load(f)['models']\n",
    "    \n",
    "    # Create a full dictionary of available models, preferring free tiers\n",
    "    available_models = {}\n",
    "    for model in models_data:\n",
    "        if model.get('type') != 'image_generation':\n",
    "            model_id = model.get('free_tier', {}).get('id', model['id'])\n",
    "            available_models[model['name']] = model_id\n",
    "            \n",
    "    # Filter the available models to get only the ones we selected\n",
    "    models_to_test = {\n",
    "        name: model_id for name, model_id in available_models.items()\n",
    "        if model_id in selected_model_ids\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Successfully loaded and selected {len(models_to_test)} models from 'models.json'.\")\n",
    "    if len(models_to_test) < len(selected_model_ids):\n",
    "        print(\"   ⚠️ Some selected models were not found in 'models.json'.\")\n",
    "\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
    "    print(f\"⚠️ Could not load models from 'models.json': {e}\")\n",
    "    print(\"   Falling back to default models.\")\n",
    "    # Fallback to a default set if the file is missing or invalid\n",
    "    models_to_test = {\n",
    "        \"OpenAI GPT-3.5\": \"openai/gpt-3.5-turbo\",\n",
    "        \"Llama3 8B\": \"meta-llama/llama-3-8b-instruct\"\n",
    "    }\n",
    "\n",
    "# Define a simple prompt\n",
    "prompt = \"Write a short, imaginative story about a librarian who discovers a book that writes itself.\"\n",
    "\n",
    "print(f\"\\n🎯 Selected models for testing:\")\n",
    "for name, model_id in models_to_test.items():\n",
    "    print(f\"   • {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeab536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Using the OpenRouter Call Function\n",
    "print(\"🎯 PRACTICAL EXAMPLE: OpenRouter Function in Action\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Example 1: Generate text for different genres using the same model\n",
    "print(\"\\n📚 Example 1: Multi-Genre Text Generation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define different prompts for various genres\n",
    "example_prompts = {\n",
    "    \"Academic\": \"Explain the concept of neuroplasticity in simple terms for students.\",\n",
    "    \"Creative\": \"Write a short paragraph about a mysterious door that appears in different places.\",\n",
    "    \"Technical\": \"Describe the basic steps for setting up a secure password policy.\",\n",
    "    \"News\": \"Write a brief news report about a new archaeological discovery.\",\n",
    "    \"Conversational\": \"Give advice to someone learning to cook their first meal.\"\n",
    "}\n",
    "\n",
    "# Select a model to use (use the first available model)\n",
    "if models_to_test:\n",
    "    selected_model_name, selected_model_id = list(models_to_test.items())[0]\n",
    "    print(f\"🤖 Using model: {selected_model_name} ({selected_model_id})\")\n",
    "    \n",
    "    # Generate text for each genre\n",
    "    generated_texts = {}\n",
    "    for genre, prompt in example_prompts.items():\n",
    "        print(f\"\\n📝 {genre} Genre:\")\n",
    "        print(f\"   Prompt: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            response = call_openrouter(prompt, selected_model_id)\n",
    "            generated_texts[genre] = response\n",
    "            \n",
    "            # Display first 100 characters of response\n",
    "            preview = response[:100] + \"...\" if len(response) > 100 else response\n",
    "            print(f\"   Response: {preview}\")\n",
    "            print(f\"   Length: {len(response)} characters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {e}\")\n",
    "            generated_texts[genre] = f\"Error: {e}\"\n",
    "    \n",
    "    # Analysis of generated texts\n",
    "    print(f\"\\n📊 Quick Analysis:\")\n",
    "    total_words = 0\n",
    "    unique_words = set()\n",
    "    \n",
    "    for genre, text in generated_texts.items():\n",
    "        if not text.startswith(\"Error:\"):\n",
    "            # Simple word extraction\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            total_words += len(words)\n",
    "            unique_words.update(words)\n",
    "            \n",
    "            print(f\"   {genre}: {len(words)} words, {len(set(words))} unique\")\n",
    "    \n",
    "    print(f\"\\n🎯 Summary:\")\n",
    "    print(f\"   Total words generated: {total_words}\")\n",
    "    print(f\"   Total unique words: {len(unique_words)}\")\n",
    "    print(f\"   Vocabulary diversity: {len(unique_words)/total_words:.3f}\" if total_words > 0 else \"   No valid text generated\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No models available for testing\")\n",
    "\n",
    "# Example 2: Compare different models with the same prompt\n",
    "print(f\"\\n\\n🔄 Example 2: Model Comparison\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if len(models_to_test) >= 2:\n",
    "    comparison_prompt = \"Describe the process of learning a new language in one paragraph.\"\n",
    "    print(f\"🎯 Prompt: {comparison_prompt}\")\n",
    "    \n",
    "    for model_name, model_id in models_to_test.items():\n",
    "        print(f\"\\n🤖 {model_name}:\")\n",
    "        try:\n",
    "            response = call_openrouter(comparison_prompt, model_id)\n",
    "            \n",
    "            # Basic analysis\n",
    "            words = re.findall(r'\\b\\w+\\b', response.lower())\n",
    "            sentences = response.count('.') + response.count('!') + response.count('?')\n",
    "            \n",
    "            print(f\"   Response: {response}\")\n",
    "            print(f\"   📊 Stats: {len(words)} words, ~{sentences} sentences\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {e}\")\n",
    "            \n",
    "elif len(models_to_test) == 1:\n",
    "    print(\"Only one model available - skipping comparison\")\n",
    "else:\n",
    "    print(\"No models available for comparison\")\n",
    "\n",
    "# Example 3: Demonstrate temperature and creativity control\n",
    "print(f\"\\n\\n🌡️ Example 3: Understanding Model Parameters\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "creativity_prompt = \"Write a creative opening line for a science fiction story.\"\n",
    "\n",
    "if models_to_test:\n",
    "    model_name, model_id = list(models_to_test.items())[0]\n",
    "    \n",
    "    print(f\"🎭 Testing creativity with: {model_name}\")\n",
    "    print(f\"📝 Prompt: {creativity_prompt}\")\n",
    "    \n",
    "    # Note: The temperature is set to 0.7 in our function\n",
    "    # In a real implementation, you might want to make this configurable\n",
    "    for i in range(3):\n",
    "        print(f\"\\n   Attempt {i+1}:\")\n",
    "        try:\n",
    "            response = call_openrouter(creativity_prompt, model_id)\n",
    "            print(f\"   → {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "print(f\"\\n✅ OpenRouter function examples complete!\")\n",
    "print(f\"💡 These examples show how to generate diverse text for corpus building.\")\n",
    "print(f\"🚀 Ready to scale up for large corpus generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06990ae8",
   "metadata": {},
   "source": [
    "### Analysis and Exploration\n",
    "\n",
    "Look at the outputs from the different models. Consider the following questions:\n",
    "- **Vocabulary**: Which model used more common words? Which used more unusual or complex words?\n",
    "- **Style**: Did one model produce a more creative or narrative style? Was another more direct or factual?\n",
    "- **Length & Structure**: Were there noticeable differences in sentence length or structure?\n",
    "\n",
    "This hands-on exploration is the first step in understanding how to build a corpus that is well-suited for psycholinguistic analysis. A good corpus needs to be diverse enough to capture the wide range of words people encounter in their daily lives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8425d7a",
   "metadata": {},
   "source": [
    "## 1.3 Large-Scale Corpus Generation (via Script)\n",
    "\n",
    "While we can generate small amounts of text interactively in this notebook, creating a multi-million word corpus requires a dedicated script that can run for a long time, save its progress, and handle potential interruptions.\n",
    "\n",
    "For this purpose, we will use `scripts/script-1-gen.py`.\n",
    "\n",
    "**How it Works:**\n",
    "1.  **Configuration File**: The script's behavior is controlled by a `.json` configuration file. We have created `scripts/config_2m_llama.json` specifically for generating our 2-million-word Llama 3.3 corpus.\n",
    "2.  **Seed vs. No Seed Words**: The script can use general, self-contained prompts across different genres to generate a diverse corpus without being biased by a specific word list or it can use seed words to \"inspire\" the LLM to write into a certain direction.\n",
    "3.  **State Management**: It continuously saves its progress to a state file (`.json`), so if the script is stopped, it can be resumed later without losing work or money.\n",
    "\n",
    "**To run the generation process, you would execute the following command in your terminal:**\n",
    "```bash\n",
    "python scripts/script-1-gen.py --config scripts/config_2m_llama.json\n",
    "```\n",
    "\n",
    "This process will run in the background to generate the `large_corpus_2m_llama.txt` file in the `output/` directory, which is essential for the next stages of our research. Note that you can change certain settings by making a new config file. For the purpose of this summer school session, a corpus has been pre-generated for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d50c74",
   "metadata": {},
   "source": [
    "## Session 1 Summary & Next Steps\n",
    "\n",
    "**What We've Accomplished:**\n",
    "- **Simplified API Access**: We set up a clean, reusable function to connect to various LLMs via OpenRouter.\n",
    "- **Interactive Exploration**: We experimented with different models and prompts to see how they affect text generation.\n",
    "- **Systematic Corpus Generation**: We created a small but diverse corpus by generating text from prompts across multiple genres.\n",
    "\n",
    "We will now move on to **Notebook 2**, where we will:\n",
    "- Load our custom-generated frequency predictor.\n",
    "- Compare it against established, human-derived frequency norms (like SUBTLEX).\n",
    "- Validate its predictive power against real human reading time data.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Open `notebook2_corpus_analysis.ipynb` to begin the analysis phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
